{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Images: 32406\n",
      "Train samples: 25924\n",
      "Validation samples: 6482\n",
      "[('data/3rd/IMG/center_2017_12_11_10_56_32_478.jpg', 0.0), ('data/2nd/IMG/center_2017_12_11_10_53_47_157.jpg', -0.2513216), ('data/3rd/IMG/right_2017_12_11_10_56_17_211.jpg', -0.5), ('data/3rd/IMG/left_2017_12_11_10_56_40_218.jpg', 0.2), ('data/0th/IMG/center_2016_12_01_13_33_25_853.jpg', 0.0), ('data/0th/IMG/left_2016_12_01_13_37_57_284.jpg', 0.3670138), ('data/0th/IMG/left_2016_12_01_13_36_54_834.jpg', 0.37658230000000004), ('data/0th/IMG/center_2016_12_01_13_42_17_637.jpg', 0.0), ('data/0th/IMG/right_2016_12_01_13_44_39_498.jpg', -0.2), ('data/0th/IMG/right_2016_12_01_13_45_18_976.jpg', -0.28824026)]\n",
      "Epoch 1/3\n",
      "25920/25924 [============================>.] - ETA: 0s - loss: 0.0477"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/engine/training.py:1569: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.\n",
      "  warnings.warn('Epoch comprised more than '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25984/25924 [==============================] - 375s - loss: 0.0477 - val_loss: 0.0399\n",
      "Epoch 2/3\n",
      "25928/25924 [==============================] - 374s - loss: 0.0433 - val_loss: 0.0421\n",
      "Epoch 3/3\n",
      "25984/25924 [==============================] - 373s - loss: 0.0391 - val_loss: 0.0359\n",
      "dict_keys(['val_loss', 'loss'])\n",
      "Loss\n",
      "[0.047658101474324914, 0.043256273710987003, 0.039095465713203541]\n",
      "Validation Loss\n",
      "[0.039938036763273621, 0.042074805948596736, 0.03587427445888227]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import sklearn\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Flatten, Dense, Lambda, Conv2D, Cropping2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "\n",
    "def drivingLogs(dataPath, skipHeader=False):\n",
    "    \"\"\"\n",
    "    Returns the lines from a driving log\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    with open(dataPath + '/driving_log.csv') as csvFile:\n",
    "        reader = csv.reader(csvFile)\n",
    "        if skipHeader:\n",
    "            next(reader, None)\n",
    "        for line in reader:\n",
    "            lines.append(line)\n",
    "    return lines\n",
    "\n",
    "def findImages(dataPath):\n",
    "    \"\"\"\n",
    "    Returns `([centerPaths], [leftPath], [rightPath], [measurement])`\n",
    "    \"\"\"\n",
    "    directories = [x[0] for x in os.walk(dataPath)]\n",
    "    dataDirectories = list(filter(lambda directory: os.path.isfile(directory + '/driving_log.csv'), directories))\n",
    "    centerTotal, leftTotal, rightTotal, measurementTotal = [], [], [], []\n",
    "    for directory in dataDirectories:\n",
    "        lines = drivingLogs(directory, skipHeader=True)\n",
    "        center, left, right, measurements = [], [], [], []\n",
    "        for line in lines:\n",
    "            measurements.append(float(line[3].strip()))\n",
    "            center.append(directory + '/' + line[0].strip())\n",
    "            left.append(directory + '/' + line[1].strip())\n",
    "            right.append(directory + '/' + line[2].strip())\n",
    "        centerTotal.extend(center)\n",
    "        leftTotal.extend(left)\n",
    "        rightTotal.extend(right)\n",
    "        measurementTotal.extend(measurements)\n",
    "\n",
    "    return (centerTotal, leftTotal, rightTotal, measurementTotal)\n",
    "\n",
    "def combineImages(center, left, right, measurement, correction):\n",
    "    \"\"\"\n",
    "    Returns ([imagePaths], [measurements])\n",
    "    \"\"\"\n",
    "    imagePaths = []\n",
    "    imagePaths.extend(center)\n",
    "    imagePaths.extend(left)\n",
    "    imagePaths.extend(right)\n",
    "    measurements = []\n",
    "    measurements.extend(measurement)\n",
    "    measurements.extend([x + correction for x in measurement])\n",
    "    measurements.extend([x - correction for x in measurement])\n",
    "    return (imagePaths, measurements)\n",
    "\n",
    "def generator(samples, batch_size=32):\n",
    "    \"\"\"\n",
    "    Generate images and measurments for training\n",
    "    \"\"\"\n",
    "    num_samples = len(samples)\n",
    "    while 1:\n",
    "        samples = sklearn.utils.shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "\n",
    "            images = []\n",
    "            angles = []\n",
    "            for imagePath, measurement in batch_samples:\n",
    "                originalImage = cv2.imread(imagePath)\n",
    "                image = cv2.cvtColor(originalImage, cv2.COLOR_BGR2RGB)\n",
    "                images.append(image)\n",
    "                angles.append(measurement)\n",
    "                # Flipping\n",
    "                images.append(cv2.flip(image,1))\n",
    "                angles.append(measurement*-1.0)\n",
    "\n",
    "            inputs = np.array(images)\n",
    "            outputs = np.array(angles)\n",
    "            yield sklearn.utils.shuffle(inputs, outputs)\n",
    "\n",
    "def createPreProcessingLayers():\n",
    "    \"\"\"\n",
    "    Creates a model with the initial pre-processing layers.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Lambda(lambda x: (x / 255.0) - 0.5, input_shape=(160,320,3)))\n",
    "    model.add(Cropping2D(cropping=((50,20), (0,0))))\n",
    "    return model\n",
    "\n",
    "def carModel():\n",
    "    \"\"\"\n",
    "    nVidea Autonomous Car model\n",
    "    \"\"\"\n",
    "    model = createPreProcessingLayers()\n",
    "    model.add(Conv2D(24,5,5, subsample=(2,2), activation='relu'))\n",
    "    model.add(Conv2D(36,5,5, subsample=(2,2), activation='relu'))\n",
    "    model.add(Conv2D(48,5,5, subsample=(2,2), activation='relu'))\n",
    "    model.add(Conv2D(64,3,3, activation='relu'))\n",
    "    model.add(Conv2D(64,3,3, activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100))\n",
    "    model.add(Dense(50))\n",
    "    model.add(Dense(10))\n",
    "    model.add(Dense(1))\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    return None\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    centerPaths, leftPaths, rightPaths, measurements = findImages('data')\n",
    "    imagePaths, measurements = combineImages(centerPaths, leftPaths, rightPaths, measurements, 0.2)\n",
    "    print('Total Images: {}'.format( len(imagePaths)))\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    samples = list(zip(imagePaths, measurements))\n",
    "    train_samples, validation_samples = train_test_split(samples, test_size=0.2)\n",
    "\n",
    "    print('Train samples: {}'.format(len(train_samples)))\n",
    "    print('Validation samples: {}'.format(len(validation_samples)))\n",
    "    \n",
    "    train_generator = generator(train_samples, batch_size=32)\n",
    "    validation_generator = generator(validation_samples, batch_size=32)\n",
    "\n",
    "    print (train_samples[:10])\n",
    "    model = carModel()\n",
    "    \n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    history_object = model.fit_generator(train_generator, samples_per_epoch= \\\n",
    "                                         len(train_samples), validation_data=validation_generator, \\\n",
    "                                         nb_val_samples=len(validation_samples), nb_epoch=3, verbose=1)\n",
    "\n",
    "    model.save('model.h5')\n",
    "    print(history_object.history.keys())\n",
    "    print('Loss')\n",
    "    print(history_object.history['loss'])\n",
    "    print('Validation Loss')\n",
    "    print(history_object.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
